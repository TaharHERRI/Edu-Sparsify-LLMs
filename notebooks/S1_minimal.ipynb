{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad99b75e",
   "metadata": {},
   "source": [
    "# S1 — Minimal: Dense → Masked → CSR\n",
    "Simple, readable baseline with robust perplexity and measured sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4785b8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os, sys, warnings, pandas as pd, torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "sys.path.append('..'); sys.path.append('../src')\n",
    "from src.eval.metrics import params_size_and_sparsity, eval_ppl_causal\n",
    "from src.eval.utils import measure_latency_ms\n",
    "from src.eval.csvlog import append_row\n",
    "from src.eval.plotting import bar_plot\n",
    "from src.pruning.policies import apply_global_magnitude_pruning_cpu_safe, select_prunable_linears\n",
    "from src.pruning.pipeline import freeze_pruning_, convert_linear_weights_to_csr_\n",
    "from src.wrappers.linear_csr import LinearCSRForward\n",
    "warnings.filterwarnings('ignore', message='.*Sparse CSR tensor support is in beta state.*')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n",
    "RESULTS_DIR = os.path.join('..','results'); CSV_PATH = os.path.join(RESULTS_DIR,'S1_minimal.csv')\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "pd.DataFrame(columns=[\"setup\",\"size_mb\",\"sparsity\",\"latency_ms\",\"perplexity\"]).to_csv(CSV_PATH, index=False)\n",
    "\n",
    "def load_fresh():\n",
    "    \"\"\"\n",
    "    Load exactly one model depending on the device:\n",
    "      - CUDA  -> EleutherAI/pythia-410m (fp16)\n",
    "      - CPU   -> facebook/opt-125m     (fp32)\n",
    "    \"\"\"\n",
    "    if device == \"cuda\":\n",
    "        model_name = \"EleutherAI/pythia-410m\"\n",
    "        torch_dtype = torch.float16\n",
    "    else:\n",
    "        model_name = \"facebook/opt-125m\"\n",
    "        torch_dtype = None  # use default (fp32)\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    tok.pad_token = tok.eos_token\n",
    "    kwargs = {}\n",
    "    if torch_dtype is not None:\n",
    "        kwargs[\"torch_dtype\"] = torch_dtype\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(model_name, **kwargs).to(device).eval()\n",
    "    print(f\"Loaded: {model_name}\")\n",
    "    return mdl, tok, model_name\n",
    "    \n",
    "def latency_fn(model, tokenizer):\n",
    "    def f(L=128, B=1):\n",
    "        inp = torch.randint(0, tokenizer.vocab_size, (B, L), device=device)\n",
    "        att = torch.ones(B, L, device=device, dtype=torch.long)\n",
    "        return model(input_ids=inp, attention_mask=att, labels=inp).logits\n",
    "    return f\n",
    "SAMPLE_TEXTS = [\n",
    "    \"In a quiet valley, the river bends slowly around the last farm before the hills.\",\n",
    "    \"Sparse pruning zeroes weights but needs a sparse kernel to speed up compute.\",\n",
    "    \"A small batch size can distort latency because of cache and warmup effects.\",\n",
    "    \"Causal LM perplexity is averaged per token over sliding blocks.\",\n",
    "    \"Version 1.2.0 fixes: stability on CPU, deterministic seeds, better logging.\",\n",
    "    \"“Hello?” — “Hi; can you hear me?” — “Loud and clear.”\",\n",
    "    \"HTTP 429 means rate limiting; use exponential backoff with jitter.\",\n",
    "    \"Compute follows memory: fewer bytes moved often means fewer milliseconds.\",\n",
    "    \"Numbers: 3.14159, 2.71828, 0.57721 show up in odd places.\",\n",
    "    \"Keep the same corpus when comparing Dense vs Masked vs CSR.\",\n",
    "    \"If latency jumps, check power limits, thermal throttling, governors.\",\n",
    "    \"We log mean, median, and p95 latency because tails matter.\",\n",
    "    \"One batch isn’t enough: run multiple iterations with warmup.\",\n",
    "    \"Tiny masking mistakes can create NaNs; clamp logits if needed.\",\n",
    "    \"When in doubt, profile with both synthetic and real inputs.\"\n",
    "]\n",
    "# Optional: virtually increase the size\n",
    "# SAMPLE_TEXTS = SAMPLE_TEXTS * 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c3ecb3",
   "metadata": {},
   "source": [
    "## 1) Dense baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bf0833b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: facebook/opt-125m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\masra\\Downloads\\sparsify-min\\notebooks\\..\\src\\eval\\csvlog.py:8: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'nonzero': 125238422,\n",
       "  'total': 125239296,\n",
       "  'sparsity': 6.978640314292406e-06,\n",
       "  'size_mb': 477.75},\n",
       " 297.2240231598208,\n",
       " 219.8335399996722)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, tok, name = load_fresh()\n",
    "stats = params_size_and_sparsity(model)\n",
    "ppl   = eval_ppl_causal(model, tok, SAMPLE_TEXTS, device)\n",
    "lat   = measure_latency_ms(latency_fn(model, tok), 128, 1, warmup=3, iters=10)\n",
    "append_row(CSV_PATH, setup='Dense', size_mb=stats['size_mb'], sparsity=stats['sparsity'], latency_ms=lat, perplexity=ppl)\n",
    "stats, ppl, lat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0098f2",
   "metadata": {},
   "source": [
    "## 2) Masked pruning (30%) — dense execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5365922b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: facebook/opt-125m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'nonzero': 125238422,\n",
       "  'total': 125239296,\n",
       "  'sparsity': 6.978640314292406e-06,\n",
       "  'size_mb': 477.75},\n",
       " 332.0806829100369,\n",
       " 320.18619000155013)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SP_MASK = 0.30\n",
    "model, tok, name = load_fresh()\n",
    "layers = select_prunable_linears(model, blacklist=(\"lm_head\",))\n",
    "apply_global_magnitude_pruning_cpu_safe(layers, amount=SP_MASK)\n",
    "stats = params_size_and_sparsity(model)\n",
    "ppl   = eval_ppl_causal(model, tok, SAMPLE_TEXTS, device)\n",
    "lat   = measure_latency_ms(latency_fn(model, tok), 128, 1, warmup=3, iters=10)\n",
    "append_row(CSV_PATH, setup=f'Masked{int(SP_MASK*100)}', size_mb=stats['size_mb'], sparsity=stats['sparsity'], latency_ms=lat, perplexity=ppl)\n",
    "stats, ppl, lat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc76a8de",
   "metadata": {},
   "source": [
    "## 3) CSR execution (30%) — real sparse kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b1330a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: facebook/opt-125m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'nonzero': 81699724,\n",
       "  'total': 122876928,\n",
       "  'sparsity': 0.3351093217434602,\n",
       "  'size_mb': 468.73828125},\n",
       " 715.6475721140738,\n",
       " 259.1533499973593)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SP_CSR = 0.30\n",
    "model, tok, name = load_fresh()\n",
    "layers = select_prunable_linears(model, blacklist=(\"lm_head\",))\n",
    "apply_global_magnitude_pruning_cpu_safe(layers, amount=SP_CSR)\n",
    "freeze_pruning_(layers); convert_linear_weights_to_csr_(layers)\n",
    "swapped = 0\n",
    "def find_parent(root, child):\n",
    "    for _, mod in root.named_modules():\n",
    "        for cn, cc in mod.named_children():\n",
    "            if cc is child: return mod, cn\n",
    "    raise RuntimeError('Parent not found')\n",
    "for lin in layers:\n",
    "    if swapped >= 4: break\n",
    "    parent, attr = find_parent(model, lin)\n",
    "    setattr(parent, attr, LinearCSRForward(lin.weight.detach(), lin.bias.detach() if lin.bias is not None else None).to(device))\n",
    "    swapped += 1\n",
    "stats = params_size_and_sparsity(model)\n",
    "ppl   = eval_ppl_causal(model, tok, SAMPLE_TEXTS, device)\n",
    "lat   = measure_latency_ms(latency_fn(model, tok), 128, 1, warmup=3, iters=10)\n",
    "append_row(CSV_PATH, setup=f'CSR{int(SP_CSR*100)}', size_mb=stats['size_mb'], sparsity=stats['sparsity'], latency_ms=lat, perplexity=ppl)\n",
    "stats, ppl, lat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba329b41",
   "metadata": {},
   "source": [
    "## 4) Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196a989c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setup</th>\n",
       "      <th>size_mb</th>\n",
       "      <th>sparsity</th>\n",
       "      <th>latency_ms</th>\n",
       "      <th>perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dense</td>\n",
       "      <td>477.750000</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>219.83354</td>\n",
       "      <td>297.224023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Masked30</td>\n",
       "      <td>477.750000</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>320.18619</td>\n",
       "      <td>332.080683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CSR50</td>\n",
       "      <td>468.738281</td>\n",
       "      <td>0.335109</td>\n",
       "      <td>259.15335</td>\n",
       "      <td>715.647572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      setup     size_mb  sparsity  latency_ms  perplexity\n",
       "0     Dense  477.750000  0.000007   219.83354  297.224023\n",
       "1  Masked30  477.750000  0.000007   320.18619  332.080683\n",
       "2     CSR50  468.738281  0.335109   259.15335  715.647572"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ..\\results\\size_vs_sparsity.png\n",
      "Saved: ..\\results\\latency_vs_sparsity.png\n",
      "Saved: ..\\results\\ppl_vs_sparsity.png\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH); display(df)\n",
    "bar_plot(df, 'setup', 'size_mb', 'Model size (MB)', 'size_vs_sparsity.png', RESULTS_DIR, y_min=700)\n",
    "bar_plot(df, 'setup', 'latency_ms', 'Latency (ms / forward)', 'latency_vs_sparsity.png', RESULTS_DIR)\n",
    "bar_plot(df, 'setup', 'perplexity', 'Perplexity', 'ppl_vs_sparsity.png', RESULTS_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
