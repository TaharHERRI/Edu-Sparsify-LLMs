{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad99b75e",
   "metadata": {
    "id": "ad99b75e"
   },
   "source": [
    "# Minimal: Dense → Masked → CSR (similarity and structure)\n",
    "\n",
    "This notebook provides a small, readable baseline comparison between three variants of the same model:\n",
    "\n",
    "- **Dense**\n",
    "- **Masked 30%** (unstructured magnitude pruning, still executed with dense kernels)\n",
    "- **CSR 30%** (same pruning, but converted to CSR and executed with sparse kernels)\n",
    "\n",
    "We compare:\n",
    "\n",
    "- **Structure:** model size (MB), global sparsity\n",
    "- **Behaviour:** *top-1 agreement* with the dense model (percentage of tokens for which the predicted class is identical).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9b356c",
   "metadata": {},
   "source": [
    "## What do the metrics mean in this notebook?\n",
    "\n",
    "This notebook compares three variants of the same model:\n",
    "\n",
    "- **Dense** – original model, no pruning.\n",
    "- **Masked 30%** – global magnitude pruning: ~30% of weights in prunable linear layers are set to zero, but tensors stay dense.\n",
    "- **CSR 30%** – same pruning level, but selected linear layers are converted to a **CSR (Compressed Sparse Row)** representation and executed with a sparse kernel (`LinearCSRForward`).\n",
    "\n",
    "For each variant we log three key metrics:\n",
    "\n",
    "### 1. `size_mb`\n",
    "\n",
    "Approximate **model size in megabytes**:\n",
    "\n",
    "- Computed from the number of parameters and their data type (fp32, fp16, …).\n",
    "- Tells you how much **memory** the model occupies on disk / in RAM / on the GPU.\n",
    "- In this notebook:\n",
    "  - **Dense** and **Masked30** have almost the same `size_mb`, because masking does **not** physically remove zeros.\n",
    "  - **CSR30** is smaller, because only the non-zero values and sparse indices are stored for converted layers.\n",
    "\n",
    "### 2. `sparsity`\n",
    "\n",
    "Global **fraction of zero weights** in the entire model:\n",
    "\n",
    "\\[\n",
    "\\text{sparsity} = 1 - \\frac{\\text{nonzero}}{\\text{total}}\n",
    "\\]\n",
    "\n",
    "- `nonzero`: number of parameters that are not equal to zero.\n",
    "- `total`: total number of parameters.\n",
    "- A higher `sparsity` means more zeros.\n",
    "- Here you can see that:\n",
    "  - **Dense** has almost zero sparsity (baseline).\n",
    "  - **Masked30** has ~20% sparsity at the model level (only some layers are pruned).\n",
    "  - **CSR30** appears almost dense again, because only CSR layers are stored sparsely; embeddings and other dense parts remain.\n",
    "\n",
    "### 3. `top1_match`\n",
    "\n",
    "**Top-1 agreement** between a variant and the dense reference on the same evaluation texts:\n",
    "\n",
    "1. We take `SAMPLE_TEXTS` (a small, fixed corpus of sentences).\n",
    "2. For each model, we compute logits over all positions.\n",
    "3. For each token position, we compare the *argmax* (most probable token) of:\n",
    "   - the dense baseline `logits_ref`, and\n",
    "   - the tested variant `logits_test`.\n",
    "4. We only count positions where the attention mask is 1 (i.e., real tokens, not padding).\n",
    "5. `top1_match` is the fraction of positions where the two models predict the **same token**:\n",
    "\n",
    "$\\text{top1\\_match} = \\mathbb{E}[\\mathbf{1}\\{ \\arg\\max \\text{Dense} = \\arg\\max \\text{Variant} \\}]$\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- `top1_match = 1.0` for **Dense** (same model vs itself).\n",
    "- A high `top1_match` for **Masked30** / **CSR30** means that pruning and CSR conversion preserve the behaviour of the dense model on this small corpus.\n",
    "- A drop in `top1_match` quantifies how much pruning changes the predictions, which complements other metrics like perplexity.\n",
    "\n",
    "This notebook is therefore mainly about:\n",
    "\n",
    "- **Structure and memory**: `size_mb`, `sparsity`.\n",
    "- **Behavioural similarity**: `top1_match` vs Dense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ILE-ENu_uJv_",
   "metadata": {
    "id": "ILE-ENu_uJv_"
   },
   "outputs": [],
   "source": [
    "# For Google Colab (comment out if you are running locally)\n",
    "# %cd /content/Edu-Sparsify-LLMs/notebooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4785b8cd",
   "metadata": {
    "id": "4785b8cd"
   },
   "outputs": [],
   "source": [
    "import os, sys, warnings, pandas as pd, torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "sys.path.append('..'); sys.path.append('../src')\n",
    "\n",
    "from src.eval.metrics import params_size_and_sparsity\n",
    "from src.eval.csvlog import append_row\n",
    "from src.eval.plotting import bar_plot\n",
    "from src.pruning.policies import apply_global_magnitude_pruning_cpu_safe, select_prunable_linears\n",
    "from src.pruning.pipeline import freeze_pruning_, convert_linear_weights_to_csr_\n",
    "from src.wrappers.linear_csr import LinearCSRForward\n",
    "\n",
    "warnings.filterwarnings('ignore', message='.*Sparse CSR tensor support is in beta state.*')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n",
    "\n",
    "RESULTS_DIR = os.path.join('..', 'results')\n",
    "CSV_PATH = os.path.join(RESULTS_DIR, 'S1_minimal_similarity.csv')\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# We log: setup, size, sparsity, top-1 similarity\n",
    "pd.DataFrame(columns=[\n",
    "    \"setup\",\n",
    "    \"size_mb\",\n",
    "    \"sparsity\",\n",
    "    \"top1_match\"\n",
    "]).to_csv(CSV_PATH, index=False)\n",
    "\n",
    "def load_fresh():\n",
    "    \"\"\"Load a small model depending on the device.\n",
    "\n",
    "    - CUDA -> EleutherAI/pythia-410m (fp16)\n",
    "    - CPU  -> facebook/opt-125m     (fp32)\n",
    "    \"\"\"\n",
    "    if device == \"cuda\":\n",
    "        model_name = \"EleutherAI/pythia-410m\"\n",
    "        torch_dtype = torch.float16\n",
    "    else:\n",
    "        model_name = \"facebook/opt-125m\"\n",
    "        torch_dtype = None  # default fp32\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "    kwargs = {}\n",
    "    if torch_dtype is not None:\n",
    "        kwargs[\"torch_dtype\"] = torch_dtype\n",
    "\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(model_name, **kwargs).to(device).eval()\n",
    "    print(f\"Loaded: {model_name}\")\n",
    "    return mdl, tok, model_name\n",
    "\n",
    "def collect_logits(model, tokenizer, texts, device):\n",
    "    \"\"\"Return (logits, attention_mask) for a list of texts.\n",
    "\n",
    "    logits: [B, L, V], mask: [B, L]\n",
    "    \"\"\"\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    enc = {k: v.to(device) for k, v in enc.items()}\n",
    "    with torch.inference_mode():\n",
    "        out = model(**enc)\n",
    "    logits = out.logits.cpu()\n",
    "    mask = enc[\"attention_mask\"].cpu()\n",
    "    return logits, mask\n",
    "\n",
    "def top1_agreement(logits_ref, logits_test, att_mask):\n",
    "    \"\"\"Percentage of tokens with identical top-1 predictions between\n",
    "    a reference model and a tested model.\n",
    "    \"\"\"\n",
    "    ref = logits_ref.argmax(dim=-1)   # [B, L]\n",
    "    test = logits_test.argmax(dim=-1) # [B, L]\n",
    "    mask = att_mask.bool()\n",
    "    match = (ref[mask] == test[mask]).float().mean().item()\n",
    "    return match\n",
    "\n",
    "SAMPLE_TEXTS = [\n",
    "    \"In a quiet valley, the river bends slowly around the last farm before the hills.\",\n",
    "    \"Sparse pruning zeroes weights but needs a sparse kernel to speed up compute.\",\n",
    "    \"A small batch size can distort latency because of cache an d warmup effects.\",\n",
    "    \"Causal LM perplexity is averaged per token over sliding blocks.\",\n",
    "    \"Version 1.2.0 fixes: stability on CPU, deterministic seeds, better logging.\",\n",
    "    \"\\\"Hello?\\\" — \\\"Hi; can you hear me?\\\" — \\\"Loud and clear.\\\"\",\n",
    "    \"HTTP 429 means rate limiting; use exponential backoff with jitter.\",\n",
    "    \"Compute follows memory: fewer bytes moved often means fewer milliseconds.\",\n",
    "    \"Numbers: 3.14159, 2.71828, 0.57721 show up in odd places.\",\n",
    "    \"Keep the same corpus when comparing Dense vs Masked vs CSR.\",\n",
    "    \"If latency jumps, check power limits, thermal throttling, governors.\",\n",
    "    \"We log mean, median, and p95 latency because tails matter.\",\n",
    "    \"One batch isn’t enough: run multiple iterations with warmup.\",\n",
    "    \"Tiny masking mistakes can create NaNs; clamp logits if needed.\",\n",
    "    \"When in doubt, profile with both synthetic and real inputs.\"\n",
    "]\n",
    "# We can virtually increase the corpus size if needed\n",
    "# SAMPLE_TEXTS = SAMPLE_TEXTS * 20\n",
    "\n",
    "# Global variables for the dense reference\n",
    "DENSE_NONZERO = None\n",
    "DENSE_LOGITS = None\n",
    "DENSE_MASK = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c3ecb3",
   "metadata": {
    "id": "b9c3ecb3"
   },
   "source": [
    "## 1) Dense baseline\n",
    "\n",
    "In this first step we:\n",
    "\n",
    "- measure the model size and (near-zero) global sparsity;\n",
    "- store:\n",
    "  - the number of non-zero parameters (`DENSE_NONZERO`),\n",
    "  - the reference logits on `SAMPLE_TEXTS` (`DENSE_LOGITS`, `DENSE_MASK`).\n",
    "\n",
    "By definition for the dense baseline:\n",
    "\n",
    "- `top1_match = 1.0` (dense vs. dense).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf0833b",
   "metadata": {
    "id": "5bf0833b"
   },
   "outputs": [],
   "source": [
    "global DENSE_NONZERO, DENSE_LOGITS, DENSE_MASK\n",
    "\n",
    "model, tok, name = load_fresh()\n",
    "stats = params_size_and_sparsity(model)\n",
    "\n",
    "# Reference logits on the small corpus\n",
    "DENSE_LOGITS, DENSE_MASK = collect_logits(model, tok, SAMPLE_TEXTS, device)\n",
    "DENSE_NONZERO = stats['nonzero']\n",
    "\n",
    "top1 = 1.0\n",
    "\n",
    "append_row(\n",
    "    CSV_PATH,\n",
    "    setup='Dense',\n",
    "    size_mb=stats['size_mb'],\n",
    "    sparsity=stats['sparsity'],\n",
    "    top1_match=top1\n",
    ")\n",
    "\n",
    "stats, top1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0098f2",
   "metadata": {
    "id": "de0098f2"
   },
   "source": [
    "## 2) Masked pruning (30%) — dense execution\n",
    "\n",
    "Here we apply **global magnitude pruning at 30%** on the selected linear layers.\n",
    "\n",
    "- The linear layers remain in **dense** format (weights are masked, but kernels are dense).\n",
    "- We measure:\n",
    "  - global sparsity,\n",
    "  - effective model size (should be ~identical to Dense),\n",
    "  - `top1_match` vs. the dense model on `SAMPLE_TEXTS`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5365922b",
   "metadata": {
    "id": "5365922b"
   },
   "outputs": [],
   "source": [
    "assert DENSE_LOGITS is not None, \"Run the Dense baseline cell first.\"\n",
    "\n",
    "SP_MASK = 0.30\n",
    "model, tok, name = load_fresh()\n",
    "\n",
    "layers = select_prunable_linears(model, blacklist=(\"lm_head\",))\n",
    "apply_global_magnitude_pruning_cpu_safe(layers, amount=SP_MASK)\n",
    "freeze_pruning_(layers)\n",
    "\n",
    "stats = params_size_and_sparsity(model)\n",
    "logits_masked, _ = collect_logits(model, tok, SAMPLE_TEXTS, device)\n",
    "\n",
    "top1 = top1_agreement(DENSE_LOGITS, logits_masked, DENSE_MASK)\n",
    "\n",
    "append_row(\n",
    "    CSV_PATH,\n",
    "    setup=f'Masked{int(SP_MASK*100)}',\n",
    "    size_mb=stats['size_mb'],\n",
    "    sparsity=stats['sparsity'],\n",
    "    top1_match=top1\n",
    ")\n",
    "\n",
    "stats, top1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc76a8de",
   "metadata": {
    "id": "dc76a8de"
   },
   "source": [
    "## 3) CSR execution (30%) — truly sparse kernels\n",
    "\n",
    "We repeat the same global 30% pruning, then:\n",
    "\n",
    "1. freeze the pruning masks,\n",
    "2. convert the pruned linear layers to CSR,\n",
    "3. replace the corresponding modules with `LinearCSRForward`.\n",
    "\n",
    "We again measure:\n",
    "\n",
    "- model size and global sparsity,\n",
    "- `top1_match` vs. the dense model on `SAMPLE_TEXTS`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b1330a",
   "metadata": {
    "id": "41b1330a"
   },
   "outputs": [],
   "source": [
    "assert DENSE_LOGITS is not None, \"Run the Dense baseline cell first.\"\n",
    "\n",
    "SP_CSR = 0.30\n",
    "model, tok, name = load_fresh()\n",
    "\n",
    "layers = select_prunable_linears(model, blacklist=(\"lm_head\",))\n",
    "apply_global_magnitude_pruning_cpu_safe(layers, amount=SP_CSR)\n",
    "freeze_pruning_(layers)\n",
    "convert_linear_weights_to_csr_(layers)\n",
    "\n",
    "# Replace all pruned linear layers by LinearCSRForward\n",
    "swapped = 0\n",
    "def find_parent(root, child):\n",
    "    for _, mod in root.named_modules():\n",
    "        for cn, cc in mod.named_children():\n",
    "            if cc is child:\n",
    "                return mod, cn\n",
    "    raise RuntimeError('Parent not found')\n",
    "\n",
    "for lin in layers:\n",
    "    parent, attr = find_parent(model, lin)\n",
    "    csr_module = LinearCSRForward(\n",
    "        lin.weight.detach(),\n",
    "        lin.bias.detach() if lin.bias is not None else None\n",
    "    ).to(device)\n",
    "    setattr(parent, attr, csr_module)\n",
    "    swapped += 1\n",
    "\n",
    "stats = params_size_and_sparsity(model)\n",
    "logits_csr, _ = collect_logits(model, tok, SAMPLE_TEXTS, device)\n",
    "\n",
    "top1 = top1_agreement(DENSE_LOGITS, logits_csr, DENSE_MASK)\n",
    "\n",
    "append_row(\n",
    "    CSV_PATH,\n",
    "    setup=f'CSR{int(SP_CSR*100)}',\n",
    "    size_mb=stats['size_mb'],\n",
    "    sparsity=stats['sparsity'],\n",
    "    top1_match=top1\n",
    ")\n",
    "\n",
    "stats, top1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba329b41",
   "metadata": {
    "id": "ba329b41"
   },
   "source": [
    "## 4) Plots\n",
    "\n",
    "We visualise the three variants (**Dense**, **Masked30**, **CSR30**) with simple bar plots:\n",
    "\n",
    "- **Model size (MB)** per setup.\n",
    "- **Global sparsity** per setup.\n",
    "- **Top-1 agreement** with the dense model per setup.\n",
    "\n",
    "These plots give a compact view of the trade-off between sparsity, model size and predictive similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196a989c",
   "metadata": {
    "id": "196a989c"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "df_sparsity = df[df[\"setup\"] != \"CSR30\"].copy()\n",
    "display(df)\n",
    "\n",
    "bar_plot(df, 'setup', 'size_mb', 'Model size (MB)', 'size_vs_setup.png', RESULTS_DIR, y_min=None)\n",
    "bar_plot(df_sparsity, 'setup', 'sparsity', 'Global sparsity', 'sparsity_vs_setup.png', RESULTS_DIR, y_min=0.0)\n",
    "bar_plot(df, 'setup', 'top1_match', 'Top-1 agreement vs Dense', 'top1_vs_sparsity.png', RESULTS_DIR, y_min=0.0)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
