{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bb854a0",
   "metadata": {},
   "source": [
    "# Structural + CSR analysis\n",
    "\n",
    "Objectives of this notebook:\n",
    "\n",
    "- **Per layer:**\n",
    "  - `nonzero`, `total`, `sparsity`\n",
    "  - dimensions (shape of the main weight tensor)\n",
    "  - estimated FLOPs per token for linear / CSR layers\n",
    "- **Per group:**\n",
    "  - `embedding`\n",
    "  - `attention_linear` (Q/K/V/out)\n",
    "  - `mlp_linear` (fully-connected layers in the MLP)\n",
    "  - `lm_head`\n",
    "  - `other_*`\n",
    "\n",
    "We compare three setups:\n",
    "\n",
    "- **Dense**\n",
    "- **Masked30** (global 30% pruning with dense execution)\n",
    "- **CSR30** (same pruning, CSR conversion for all prunable linears)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fb8ed9",
   "metadata": {},
   "source": [
    "# What the metrics mean in this notebook (clear explanation)\n",
    "\n",
    "This notebook performs a **structural analysis** of a Transformer model for three variants:\n",
    "\n",
    "* **Dense** — original model\n",
    "* **Masked30** — 30% global magnitude pruning (weights set to 0, tensors stay dense)\n",
    "* **CSR30** — same pruning, but selected Linear layers are replaced by `LinearCSRForward` (true sparse CSR matrices)\n",
    "\n",
    "The goal is to understand:\n",
    "-> **Where parameters live**\n",
    "-> **Where sparsity appears**\n",
    "-> **How much theoretical compute is removed**\n",
    "-> **How the structure differs across Dense / Masked / CSR**\n",
    "\n",
    "---\n",
    "\n",
    "## Per-layer metrics (`df_layers_*`)\n",
    "\n",
    "Each **row** in `df_layers` corresponds to **one module inside the model**.\n",
    "Example rows:\n",
    "\n",
    "* `model.decoder.layers.0.self_attn.q_proj`\n",
    "* `model.decoder.layers.3.mlp.fc1`\n",
    "* `model.decoder.embed_tokens`\n",
    "* `lm_head`\n",
    "* etc.\n",
    "\n",
    "For each module we record:\n",
    "\n",
    "### **Basic identifiers**\n",
    "\n",
    "* **`model`** — which variant this row belongs to (`Dense`, `Masked30`, `CSR30`)\n",
    "* **`module_name`** — full dotted name inside the model\n",
    "* **`group`** — high-level category:\n",
    "\n",
    "  * `embedding`\n",
    "  * `attention_linear` (q/k/v/out projections)\n",
    "  * `mlp_linear` (feed-forward layers)\n",
    "  * `lm_head`\n",
    "  * `norm`\n",
    "  * `other_linear` / `other`\n",
    "\n",
    "### **Parameter statistics**\n",
    "\n",
    "* **`nonzero`** — number of parameters ≠ 0\n",
    "* **`total`** — total parameters\n",
    "* **`sparsity`** — fraction of zeros:\n",
    "\n",
    "$\\text{sparsity} = 1 - \\frac{\\text{nonzero}}{\\text{total}}$\n",
    "\n",
    "* **`shape`** — weight matrix shape (e.g. `(out, in)` for Linear)\n",
    "\n",
    "### **Compute estimate**\n",
    "\n",
    "* **Dense Linear**:\n",
    "  FLOPs per token ≈ `2 * in_features * out_features`\n",
    "* **CSR Linear**:\n",
    "  FLOPs per token ≈ `2 * nnz`\n",
    "  (only non-zeros count)\n",
    "\n",
    "### **Parameter share**\n",
    "\n",
    "* **`param_frac`** — proportion of model parameters in that module:\n",
    "\n",
    "$\\text{param\\_frac} = \n",
    "\\frac{\\text{total\\ parameters\\ in\\ layer}}\n",
    "     {\\text{total\\ parameters\\ in\\ model}}$\n",
    "\n",
    "\n",
    "-> This lets you zoom in on **specific layers** and see how pruning alters their structure.\n",
    "\n",
    "---\n",
    "\n",
    "## Group-level metrics (`df_groups_*`)\n",
    "\n",
    "We aggregate the layer-level stats by `(model, group)`.\n",
    "\n",
    "For each group (e.g., all attention projections):\n",
    "\n",
    "* **`nonzero`** — sum of non-zeros across modules\n",
    "* **`total`** — total parameters\n",
    "* **`sparsity`**:\n",
    "\n",
    "$\\text{sparsity}*{\\text{group}} = 1 - \\frac{\\text{nonzero}*{\\text{group}}}{\\text{total}_{\\text{group}}}$\n",
    "\n",
    "* **`flops_per_token`** — sum of FLOPs for all modules in this group\n",
    "* **`param_frac`** — fraction of the model’s parameters inside this group\n",
    "\n",
    "-> This highlights which *parts of the network* dominate size and compute.\n",
    "\n",
    "---\n",
    "\n",
    "## How to interpret the plots\n",
    "\n",
    "The notebook generates three comparisons across **Dense / Masked30 / CSR30**:\n",
    "\n",
    "### **1. Group sparsity**\n",
    "\n",
    "Shows **where zeroing actually happens**.\n",
    "\n",
    "* Embeddings / lm_head remain dense (excluded from pruning)\n",
    "* Attention and MLP linear layers become sparse\n",
    "\n",
    "### **2. Parameter share**\n",
    "\n",
    "Tells you **where parameters naturally live**, regardless of sparsity.\n",
    "\n",
    "* MLP layers → large fraction\n",
    "* Attention projections → significant\n",
    "* Embeddings → often huge chunk in small LLMs\n",
    "\n",
    "### **3. FLOPs per token**\n",
    "\n",
    "A **theoretical compute estimate**:\n",
    "\n",
    "* Dense compute ∝ number of weight multiplications\n",
    "* CSR compute ∝ number of non-zeros\n",
    "* Masked compute ≈ Dense compute (dense kernels ignore zeros)\n",
    "\n",
    "This illustrates:\n",
    "\n",
    "* Masked pruning is **structural sparsity only** (no speedup)\n",
    "* CSR pruning is **algorithmic sparsity** (real reduction in multiply-adds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "246927f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, warnings, pandas as pd, torch, torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append('..'); sys.path.append('../src')\n",
    "\n",
    "from src.eval.metrics import params_size_and_sparsity\n",
    "from src.pruning.policies import apply_global_magnitude_pruning_cpu_safe, select_prunable_linears\n",
    "from src.pruning.pipeline import freeze_pruning_, convert_linear_weights_to_csr_\n",
    "from src.wrappers.linear_csr import LinearCSRForward\n",
    "\n",
    "warnings.filterwarnings('ignore', message='.*Sparse CSR tensor support is in beta state.*')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n",
    "\n",
    "RESULTS_DIR = os.path.join('..', 'results')\n",
    "STRUCT_DIR = os.path.join(RESULTS_DIR, 'structural_layers')\n",
    "os.makedirs(STRUCT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e54320f",
   "metadata": {},
   "source": [
    "## 1. Model loading\n",
    "\n",
    "We load a small model depending on the device:\n",
    "\n",
    "- On **GPU**: `EleutherAI/pythia-410m` in fp16\n",
    "- On **CPU**: `facebook/opt-125m` in fp32\n",
    "\n",
    "If you want to use a local snapshot (for example on Narval), simply replace `model_name` with the local path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13cca1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fresh():\n",
    "    \"\"\"\n",
    "    Load a small model depending on the device.\n",
    "\n",
    "    - CUDA -> EleutherAI/pythia-410m (fp16)\n",
    "    - CPU  -> facebook/opt-125m     (fp32)\n",
    "    \"\"\"\n",
    "    if device == \"cuda\":\n",
    "        model_name = \"EleutherAI/pythia-410m\"\n",
    "        torch_dtype = torch.float16\n",
    "    else:\n",
    "        model_name = \"facebook/opt-125m\"\n",
    "        torch_dtype = None  # fp32\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    kwargs = {}\n",
    "    if torch_dtype is not None:\n",
    "        kwargs[\"torch_dtype\"] = torch_dtype\n",
    "\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        **kwargs\n",
    "    ).to(device).eval()\n",
    "    print(f\"Loaded: {model_name}\")\n",
    "    return mdl, tok, model_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a55f91b",
   "metadata": {},
   "source": [
    "## 2. Per-layer analysis helpers\n",
    "\n",
    "We define the following utilities:\n",
    "\n",
    "- `tensor_stats(t)` → `(nonzero, total)`\n",
    "- `linear_flops(weight)` → approximate FLOPs per token for a linear layer (≈ `2 * in * out`)\n",
    "- `classify_module(name, module)` → assign each module to a group (`embedding`, `attention_linear`, `mlp_linear`, `lm_head`, etc.)\n",
    "- `analyze_layers(model, label)` → returns:\n",
    "  - a *per-layer* DataFrame, and\n",
    "  - a *per-group* aggregation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f40a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_stats(t: torch.Tensor):\n",
    "    if t.numel() == 0:\n",
    "        return 0, 0\n",
    "    nnz = int((t != 0).sum().item())\n",
    "    total = t.numel()\n",
    "    return nnz, total\n",
    "\n",
    "def linear_flops(weight: torch.Tensor):\n",
    "    \"\"\"Approximate FLOPs per token for a linear layer.\n",
    "    We count 2 * in * out (mul + add).\"\"\"\n",
    "    if weight is None or weight.dim() != 2:\n",
    "        return 0\n",
    "    out_features, in_features = weight.shape\n",
    "    return int(2 * in_features * out_features)\n",
    "\n",
    "def classify_module(name: str, module: nn.Module) -> str:\n",
    "    lname = name.lower()\n",
    "\n",
    "    if isinstance(module, nn.Embedding) or 'embed' in lname:\n",
    "        return 'embedding'\n",
    "\n",
    "    if 'lm_head' in lname:\n",
    "        return 'lm_head'\n",
    "\n",
    "    if isinstance(module, (nn.Linear, LinearCSRForward)):\n",
    "        # Attention\n",
    "        if 'attn' in lname or 'attention' in lname or 'self_attn' in lname:\n",
    "            return 'attention_linear'\n",
    "        # MLP\n",
    "        if 'mlp' in lname or 'ff' in lname or 'fc1' in lname or 'fc2' in lname:\n",
    "            return 'mlp_linear'\n",
    "        return 'other_linear'\n",
    "\n",
    "    if 'norm' in lname:\n",
    "        return 'norm'\n",
    "\n",
    "    return 'other'\n",
    "\n",
    "def analyze_layers(model: nn.Module, label: str):\n",
    "    rows = []\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if name == '':\n",
    "            continue\n",
    "\n",
    "        nonzero = 0\n",
    "        total = 0\n",
    "        flops = 0\n",
    "        shape = None\n",
    "\n",
    "        if isinstance(module, LinearCSRForward):\n",
    "            # Read CSR meta-data\n",
    "            nonzero = module.meta_nnz\n",
    "            total = module.meta_total_params\n",
    "            sparsity = module.meta_sparsity\n",
    "            shape = tuple(module.meta_dense_shape.tolist())\n",
    "            # FLOPs ≈ 2 * nnz (mul + add)\n",
    "            flops = 2 * nonzero\n",
    "        else:\n",
    "            # Standard path (nn.Linear, Embedding, etc.)\n",
    "            for p_name, p in module.named_parameters(recurse=False):\n",
    "                if not isinstance(p, torch.Tensor):\n",
    "                    continue\n",
    "                nnz, tot = tensor_stats(p)\n",
    "                nonzero += nnz\n",
    "                total += tot\n",
    "                if p_name in ('weight', 'weight_orig'):\n",
    "                    flops += linear_flops(p)\n",
    "                    if p.dim() == 2:\n",
    "                        shape = tuple(p.shape)\n",
    "\n",
    "            if total == 0:\n",
    "                continue\n",
    "            sparsity = 1.0 - nonzero / total\n",
    "\n",
    "        group = classify_module(name, module)\n",
    "        rows.append({\n",
    "            'model': label,\n",
    "            'module_name': name,\n",
    "            'group': group,\n",
    "            'nonzero': nonzero,\n",
    "            'total': total,\n",
    "            'sparsity': sparsity,\n",
    "            'shape': str(shape) if shape is not None else '',\n",
    "            'flops_per_token': flops\n",
    "        })\n",
    "\n",
    "    df_layers = pd.DataFrame(rows)\n",
    "    total_params_model = df_layers['total'].sum()\n",
    "    df_layers['param_frac'] = df_layers['total'] / total_params_model\n",
    "\n",
    "    df_groups = (\n",
    "        df_layers\n",
    "        .groupby(['model', 'group'], as_index=False)\n",
    "        .agg({'nonzero': 'sum', 'total': 'sum', 'flops_per_token': 'sum'})\n",
    "    )\n",
    "    df_groups['sparsity'] = 1.0 - df_groups['nonzero'] / df_groups['total']\n",
    "    df_groups['param_frac'] = df_groups['total'] / total_params_model\n",
    "\n",
    "    return df_layers, df_groups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35327385",
   "metadata": {},
   "source": [
    "## 3. Dense — baseline structure\n",
    "\n",
    "We first analyse the **unpruned dense model**.\n",
    "\n",
    "For this variant we compute for each layer and group:\n",
    "\n",
    "- number of non-zero parameters,\n",
    "- total parameter count,\n",
    "- sparsity,\n",
    "- parameter fraction within the model,\n",
    "- estimated FLOPs per token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8d82a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dense, tok, model_name = load_fresh()\n",
    "dense_stats = params_size_and_sparsity(model_dense)\n",
    "print('Dense global stats:', dense_stats)\n",
    "\n",
    "df_layers_dense, df_groups_dense = analyze_layers(model_dense, 'Dense')\n",
    "display(df_groups_dense.sort_values('total', ascending=False))\n",
    "\n",
    "dense_layers_csv = os.path.join(STRUCT_DIR, 'layers_dense.csv')\n",
    "dense_groups_csv = os.path.join(STRUCT_DIR, 'groups_dense.csv')\n",
    "df_layers_dense.to_csv(dense_layers_csv, index=False)\n",
    "df_groups_dense.to_csv(dense_groups_csv, index=False)\n",
    "print('Saved:', dense_layers_csv)\n",
    "print('Saved:', dense_groups_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd3ac32",
   "metadata": {},
   "source": [
    "## 4. Masked30 — global magnitude pruning (30%) on prunable linears\n",
    "\n",
    "We now apply global magnitude pruning (30%) on the prunable linear layers:\n",
    "\n",
    "- We prune only the `nn.Linear` modules returned by `select_prunable_linears`.\n",
    "- The `lm_head` is blacklisted.\n",
    "- Embeddings and LayerNorms are never pruned.\n",
    "\n",
    "We then inspect the effect on:\n",
    "\n",
    "- sparsity per layer / per group,\n",
    "- theoretical FLOPs per token per group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "100574b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SP = 0.30\n",
    "\n",
    "model_masked, tok_m, _ = load_fresh()\n",
    "layers_prunable = select_prunable_linears(model_masked, blacklist=(\"lm_head\",))\n",
    "print('Prunable linear layers:', len(layers_prunable))\n",
    "\n",
    "apply_global_magnitude_pruning_cpu_safe(layers_prunable, amount=SP)\n",
    "freeze_pruning_(layers_prunable)\n",
    "\n",
    "masked_stats = params_size_and_sparsity(model_masked)\n",
    "print('Masked global stats:', masked_stats)\n",
    "\n",
    "df_layers_masked, df_groups_masked = analyze_layers(model_masked, f'Masked{int(SP*100)}')\n",
    "display(df_groups_masked.sort_values('total', ascending=False))\n",
    "\n",
    "masked_layers_csv = os.path.join(STRUCT_DIR, 'layers_masked30.csv')\n",
    "masked_groups_csv = os.path.join(STRUCT_DIR, 'groups_masked30.csv')\n",
    "df_layers_masked.to_csv(masked_layers_csv, index=False)\n",
    "df_groups_masked.to_csv(masked_groups_csv, index=False)\n",
    "print('Saved:', masked_layers_csv)\n",
    "print('Saved:', masked_groups_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f8741d",
   "metadata": {},
   "source": [
    "## 5. CSR30 — pruning + CSR conversion (GPU-ready)\n",
    "\n",
    "The pipeline for the CSR variant is:\n",
    "\n",
    "1. Load a fresh model.\n",
    "2. Apply the same global 30% pruning on prunable linear layers.\n",
    "3. Call `freeze_pruning_` to materialise the masks.\n",
    "4. Run `convert_linear_weights_to_csr_`.\n",
    "5. Replace all pruned linear layers by `LinearCSRForward` modules.\n",
    "\n",
    "We then repeat the layer- and group-level analysis.\n",
    "\n",
    "> On CPU this is slower, but on GPU this corresponds to the “CSR, GPU-ready” configuration that we will benchmark on Narval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28f42487",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_csr, tok_c, _ = load_fresh()\n",
    "layers_prunable_csr = select_prunable_linears(model_csr, blacklist=(\"lm_head\",))\n",
    "print('Prunable linear layers (CSR):', len(layers_prunable_csr))\n",
    "\n",
    "apply_global_magnitude_pruning_cpu_safe(layers_prunable_csr, amount=SP)\n",
    "freeze_pruning_(layers_prunable_csr)\n",
    "convert_linear_weights_to_csr_(layers_prunable_csr)\n",
    "\n",
    "# Replace all pruned layers with LinearCSRForward\n",
    "def find_parent(root, child):\n",
    "    for _, mod in root.named_modules():\n",
    "        for cn, cc in mod.named_children():\n",
    "            if cc is child:\n",
    "                return mod, cn\n",
    "    raise RuntimeError('Parent not found')\n",
    "\n",
    "for lin in layers_prunable_csr:\n",
    "    parent, attr = find_parent(model_csr, lin)\n",
    "    csr_module = LinearCSRForward(\n",
    "        lin.weight.detach(),\n",
    "        lin.bias.detach() if lin.bias is not None else None\n",
    "    ).to(device)\n",
    "    setattr(parent, attr, csr_module)\n",
    "\n",
    "csr_stats = params_size_and_sparsity(model_csr)\n",
    "print('CSR global stats:', csr_stats)\n",
    "\n",
    "df_layers_csr, df_groups_csr = analyze_layers(model_csr, f'CSR{int(SP*100)}')\n",
    "display(df_groups_csr.sort_values('total', ascending=False))\n",
    "\n",
    "csr_layers_csv = os.path.join(STRUCT_DIR, 'layers_csr30.csv')\n",
    "csr_groups_csv = os.path.join(STRUCT_DIR, 'groups_csr30.csv')\n",
    "df_layers_csr.to_csv(csr_layers_csv, index=False)\n",
    "df_groups_csr.to_csv(csr_groups_csv, index=False)\n",
    "print('Saved:', csr_layers_csv)\n",
    "print('Saved:', csr_groups_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160b56d7",
   "metadata": {},
   "source": [
    "## 6. Global comparison: Dense vs Masked30 vs CSR30\n",
    "\n",
    "We merge the per-group DataFrames to build an overview with:\n",
    "\n",
    "- sparsity per group and per variant,\n",
    "- parameter share per group and per variant,\n",
    "- total FLOPs per token per group and per variant.\n",
    "\n",
    "This makes it easy to see where parameters and compute are concentrated, and how pruning + CSR change the picture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e867b7a3-0e38-4b31-862f-ef80d3957713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grouped(pivot_df, title, ylabel, filename, fmt='{:.3f}'):\n",
    "    groups = list(pivot_df.index)\n",
    "    variants = list(pivot_df.columns)\n",
    "    x = list(range(len(groups)))\n",
    "    width = 0.8 / max(1, len(variants))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    for i, var in enumerate(variants):\n",
    "        values = pivot_df[var].values\n",
    "        offset = (i - (len(variants) - 1) / 2) * width\n",
    "        xs = [xi + offset for xi in x]\n",
    "        bars = ax.bar(xs, values, width, label=var)\n",
    "        # annotations\n",
    "        for bx, bv in zip(xs, values):\n",
    "            ax.text(bx, bv, fmt.format(bv), ha='center', va='bottom', fontsize=8, rotation=0)\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(groups, rotation=30, ha='right')\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    path = os.path.join(STRUCT_DIR, filename)\n",
    "    fig.savefig(path, dpi=150)\n",
    "    plt.close(fig)\n",
    "    print('Saved plot:', path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da26530e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg_d = df_groups_dense.copy();   dfg_d['variant'] = 'Dense'\n",
    "dfg_m = df_groups_masked.copy();  dfg_m['variant'] = 'Masked30'\n",
    "dfg_c = df_groups_csr.copy();     dfg_c['variant'] = 'CSR30'\n",
    "\n",
    "dfg_all = pd.concat([dfg_d, dfg_m, dfg_c], ignore_index=True)\n",
    "# dfg_all = pd.concat([dfg_d, dfg_m], ignore_index=True)\n",
    "display(dfg_all.sort_values(['group', 'variant']))\n",
    "\n",
    "pivot_sparsity = dfg_all.pivot(index='group', columns='variant', values='sparsity').fillna(0.0)\n",
    "pivot_frac = dfg_all.pivot(index='group', columns='variant', values='param_frac').fillna(0.0)\n",
    "pivot_flops = dfg_all.pivot(index='group', columns='variant', values='flops_per_token').fillna(0.0)\n",
    "\n",
    "print('\\nSparsity per group:')\n",
    "display(pivot_sparsity)\n",
    "print('\\nParameter share per group:')\n",
    "display(pivot_frac)\n",
    "print('\\nFLOPs per token per group:')\n",
    "display(pivot_flops)\n",
    "\n",
    "# 3 plots globaux avec infos riches\n",
    "plot_grouped(pivot_sparsity, 'Sparsity per group (Dense vs Masked30 vs CSR30)', 'sparsity', 'all_sparsity_per_group.png', fmt='{:.3f}')\n",
    "plot_grouped(pivot_frac, 'Parameter share per group', 'parameter fraction', 'all_param_frac_per_group.png', fmt='{:.3f}')\n",
    "plot_grouped(pivot_flops, 'FLOPs per token per group', 'FLOPs per token', 'all_flops_per_group.png', fmt='{:.0f}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
