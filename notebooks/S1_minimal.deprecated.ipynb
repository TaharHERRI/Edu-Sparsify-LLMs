{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad99b75e",
   "metadata": {
    "id": "ad99b75e"
   },
   "source": [
    "# S1 — Minimal: Dense → Masked → CSR\n",
    "Simple, readable baseline with robust perplexity and measured sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ILE-ENu_uJv_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ILE-ENu_uJv_",
    "outputId": "b9599595-d435-484b-d61f-a63e7ff6c54d"
   },
   "outputs": [],
   "source": [
    "#For Google Colab\n",
    "#%cd /content/Edu-Sparsify-LLMs/notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4785b8cd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4785b8cd",
    "outputId": "cdcba82c-1ac4-440d-ee2c-122dbd5bb84b"
   },
   "outputs": [],
   "source": [
    "import os, sys, warnings, pandas as pd, torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "sys.path.append('..'); sys.path.append('../src')\n",
    "from src.eval.metrics import params_size_and_sparsity, eval_ppl_causal\n",
    "from src.eval.utils import measure_latency_ms\n",
    "from src.eval.csvlog import append_row\n",
    "from src.eval.plotting import bar_plot\n",
    "from src.pruning.policies import apply_global_magnitude_pruning_cpu_safe, select_prunable_linears\n",
    "from src.pruning.pipeline import freeze_pruning_, convert_linear_weights_to_csr_\n",
    "from src.wrappers.linear_csr import LinearCSRForward\n",
    "warnings.filterwarnings('ignore', message='.*Sparse CSR tensor support is in beta state.*')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n",
    "RESULTS_DIR = os.path.join('..','results'); CSV_PATH = os.path.join(RESULTS_DIR,'S1_minimal.csv')\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "pd.DataFrame(columns=[\"setup\",\"size_mb\",\"sparsity\",\"latency_ms\",\"perplexity\"]).to_csv(CSV_PATH, index=False)\n",
    "\n",
    "def load_fresh():\n",
    "    \"\"\"\n",
    "    Load exactly one model depending on the device:\n",
    "      - CUDA  -> EleutherAI/pythia-410m (fp16)\n",
    "      - CPU   -> facebook/opt-125m     (fp32)\n",
    "    \"\"\"\n",
    "    if device == \"cuda\":\n",
    "        model_name = \"EleutherAI/pythia-410m\"\n",
    "        torch_dtype = torch.float16\n",
    "    else:\n",
    "        model_name = \"facebook/opt-125m\"\n",
    "        torch_dtype = None  # use default (fp32)\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    tok.pad_token = tok.eos_token\n",
    "    kwargs = {}\n",
    "    if torch_dtype is not None:\n",
    "        kwargs[\"torch_dtype\"] = torch_dtype\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(model_name, **kwargs).to(device).eval()\n",
    "    print(f\"Loaded: {model_name}\")\n",
    "    return mdl, tok, model_name\n",
    "\n",
    "def latency_fn(model, tokenizer, gen_tokens=64):\n",
    "    def f(L=128, B=1):\n",
    "        inp = torch.randint(0, tokenizer.vocab_size, (B, L), device=device)\n",
    "        att = torch.ones(B, L, device=device, dtype=torch.long)\n",
    "\n",
    "        # Sécurité: pad/eos pour generate\n",
    "        pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "        eos_id = tokenizer.eos_token_id\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            _ = model.generate(\n",
    "                input_ids=inp,\n",
    "                attention_mask=att,\n",
    "                max_new_tokens=gen_tokens,\n",
    "                do_sample=False,\n",
    "                use_cache=True,\n",
    "                pad_token_id=pad_id,\n",
    "                eos_token_id=eos_id\n",
    "            )\n",
    "        # measure_latency_ms attend juste que f(L,B) \"fasse le travail\"\n",
    "        return torch.empty(())  # tensor trivial pour garder l'API\n",
    "    return f\n",
    "SAMPLE_TEXTS = [\n",
    "    \"In a quiet valley, the river bends slowly around the last farm before the hills.\",\n",
    "    \"Sparse pruning zeroes weights but needs a sparse kernel to speed up compute.\",\n",
    "    \"A small batch size can distort latency because of cache and warmup effects.\",\n",
    "    \"Causal LM perplexity is averaged per token over sliding blocks.\",\n",
    "    \"Version 1.2.0 fixes: stability on CPU, deterministic seeds, better logging.\",\n",
    "    \"“Hello?” — “Hi; can you hear me?” — “Loud and clear.”\",\n",
    "    \"HTTP 429 means rate limiting; use exponential backoff with jitter.\",\n",
    "    \"Compute follows memory: fewer bytes moved often means fewer milliseconds.\",\n",
    "    \"Numbers: 3.14159, 2.71828, 0.57721 show up in odd places.\",\n",
    "    \"Keep the same corpus when comparing Dense vs Masked vs CSR.\",\n",
    "    \"If latency jumps, check power limits, thermal throttling, governors.\",\n",
    "    \"We log mean, median, and p95 latency because tails matter.\",\n",
    "    \"One batch isn’t enough: run multiple iterations with warmup.\",\n",
    "    \"Tiny masking mistakes can create NaNs; clamp logits if needed.\",\n",
    "    \"When in doubt, profile with both synthetic and real inputs.\"\n",
    "]\n",
    "# Optional: virtually increase the size\n",
    "# SAMPLE_TEXTS = SAMPLE_TEXTS * 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c3ecb3",
   "metadata": {
    "id": "b9c3ecb3"
   },
   "source": [
    "## 1) Dense baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf0833b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5bf0833b",
    "outputId": "dae4585c-3e1c-4a54-e1b3-b0a7e5943ead"
   },
   "outputs": [],
   "source": [
    "model, tok, name = load_fresh()\n",
    "stats = params_size_and_sparsity(model)\n",
    "ppl   = eval_ppl_causal(model, tok, SAMPLE_TEXTS, device)\n",
    "lat   = measure_latency_ms(latency_fn(model, tok, gen_tokens=64), 512, 4, warmup=10, iters=30)\n",
    "append_row(CSV_PATH, setup='Dense', size_mb=stats['size_mb'], sparsity=stats['sparsity'], latency_ms=lat, perplexity=ppl)\n",
    "stats, ppl, lat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0098f2",
   "metadata": {
    "id": "de0098f2"
   },
   "source": [
    "## 2) Masked pruning (30%) — dense execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5365922b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5365922b",
    "outputId": "efbcbd99-511e-4d22-ff67-d9560a65488d"
   },
   "outputs": [],
   "source": [
    "SP_MASK = 0.30\n",
    "model, tok, name = load_fresh()\n",
    "layers = select_prunable_linears(model, blacklist=(\"lm_head\",))\n",
    "apply_global_magnitude_pruning_cpu_safe(layers, amount=SP_MASK)\n",
    "freeze_pruning_(layers)\n",
    "stats = params_size_and_sparsity(model)\n",
    "ppl   = eval_ppl_causal(model, tok, SAMPLE_TEXTS, device)\n",
    "lat   = measure_latency_ms(latency_fn(model, tok, gen_tokens=64), 512, 4, warmup=10, iters=30)\n",
    "append_row(CSV_PATH, setup=f'Masked{int(SP_MASK*100)}', size_mb=stats['size_mb'], sparsity=stats['sparsity'], latency_ms=lat, perplexity=ppl)\n",
    "stats, ppl, lat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc76a8de",
   "metadata": {
    "id": "dc76a8de"
   },
   "source": [
    "## 3) CSR execution (30%) — real sparse kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b1330a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "41b1330a",
    "outputId": "91c9aa11-9417-4d80-ecbe-1bd585761dbf"
   },
   "outputs": [],
   "source": [
    "SP_CSR = 0.30\n",
    "model, tok, name = load_fresh()\n",
    "layers = select_prunable_linears(model, blacklist=(\"lm_head\",))\n",
    "apply_global_magnitude_pruning_cpu_safe(layers, amount=SP_CSR)\n",
    "freeze_pruning_(layers); convert_linear_weights_to_csr_(layers)\n",
    "swapped = 0\n",
    "def find_parent(root, child):\n",
    "    for _, mod in root.named_modules():\n",
    "        for cn, cc in mod.named_children():\n",
    "            if cc is child: return mod, cn\n",
    "    raise RuntimeError('Parent not found')\n",
    "for lin in layers:\n",
    "    if swapped >= 4: break\n",
    "    parent, attr = find_parent(model, lin)\n",
    "    setattr(parent, attr, LinearCSRForward(lin.weight.detach(), lin.bias.detach() if lin.bias is not None else None).to(device))\n",
    "    swapped += 1\n",
    "stats = params_size_and_sparsity(model)\n",
    "ppl   = eval_ppl_causal(model, tok, SAMPLE_TEXTS, device)\n",
    "lat   = measure_latency_ms(latency_fn(model, tok, gen_tokens=64), 512, 4, warmup=10, iters=30)\n",
    "append_row(CSV_PATH, setup=f'CSR{int(SP_CSR*100)}', size_mb=stats['size_mb'], sparsity=stats['sparsity'], latency_ms=lat, perplexity=ppl)\n",
    "stats, ppl, lat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba329b41",
   "metadata": {
    "id": "ba329b41"
   },
   "source": [
    "## 4) Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196a989c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "id": "196a989c",
    "outputId": "1ddad7a6-020e-4e9b-bec3-b3396b85e007"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_PATH); display(df)\n",
    "bar_plot(df, 'setup', 'size_mb', 'Model size (MB)', 'size_vs_sparsity.png', RESULTS_DIR, y_min=400)\n",
    "bar_plot(df, 'setup', 'latency_ms', 'Latency (ms / forward)', 'latency_vs_sparsity.png', RESULTS_DIR)\n",
    "bar_plot(df, 'setup', 'perplexity', 'Perplexity', 'ppl_vs_sparsity.png', RESULTS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50f4e77-81ea-44ce-8fe6-9fa04f06fd7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
